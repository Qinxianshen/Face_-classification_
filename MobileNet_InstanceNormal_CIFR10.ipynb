{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\anaconda3.4\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-  \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tf_contrib\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import _pickle as pickle\n",
    "\n",
    "\n",
    "from skimage import io, transform\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#初始的定义的数据\n",
    "class_num = 10\n",
    "image_size = 32\n",
    "batch_size = 16\n",
    "iterations = 10\n",
    "total_epoch = 1\n",
    "\n",
    "#外部参数\n",
    "experiment_dir = \"test_experiment\"\n",
    "num_epochs = 100\n",
    "num_classes = 1001\n",
    "width_multiplier = 1.0\n",
    "shuffle = True\n",
    "l2_strength = 4e-5\n",
    "bias = np.float64(0)\n",
    "learning_rate = 1e-3\n",
    "batchnorm_enabled = True\n",
    "dropout_keep_prob = 0.999\n",
    "pretrained_path = \"pretrained_weights/mobilenet_v1.pkl\"\n",
    "max_to_keep = 4\n",
    "save_model_every = 5\n",
    "test_every = 5\n",
    "to_train = True\n",
    "to_test = False\n",
    "\n",
    "# init parameters and input\n",
    "X = None\n",
    "y = None\n",
    "logits = None\n",
    "is_training = None\n",
    "loss = None\n",
    "regularization_loss = None\n",
    "cross_entropy_loss = None\n",
    "train_op = None\n",
    "accuracy = None\n",
    "y_out_argmax = None\n",
    "summaries_merged = None\n",
    "# mean_img = None\n",
    "nodes = dict()\n",
    "\n",
    "pretrained_path = os.path.realpath(pretrained_path)\n",
    "\n",
    "# 数据集地址  最后的斜杆一定不要漏\n",
    "path = 'D:/QIN/Face-classfication/TestDatabase_1/'\n",
    "# 处理后的数据集\n",
    "path_normalization = 'D:/QIN/Face-classfication/TestDatabase_2/'\n",
    "# 模型保存地址\n",
    "model_path = './人脸识别/model.ckpt'\n",
    "# tfrecord文件存放路径\n",
    "TFRECORD_FILE = \"./人脸识别/tfrecords/\"\n",
    "\n",
    "#全局one-hot编码空间\n",
    "label_binarizer = \"\"\n",
    "\n",
    "\n",
    "\n",
    "def batch_norm(input):\n",
    "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3,\n",
    "                                        is_training=train_flag, updates_collections=None)\n",
    "\n",
    "\n",
    "def instance_norm(x, scope='instance_norm'):\n",
    "    return tf_contrib.layers.instance_norm(x,\n",
    "                                           epsilon=1e-05,\n",
    "                                           center=True, scale=True,\n",
    "                                           scope=scope)\n",
    "\n",
    "def layer_norm(x, scope='layer_norm') :\n",
    "    return tf_contrib.layers.layer_norm(x,\n",
    "                                        center=True, scale=True,\n",
    "                                        scope=scope)\n",
    "\n",
    "def norm(x, norm_type, is_train,i, G=32, esp=1e-5):\n",
    "    with tf.variable_scope('{}_norm_{}'.format(norm_type,i)):\n",
    "        if norm_type == 'none':\n",
    "            output = x\n",
    "        elif norm_type == 'batch':\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "                x, center=True, scale=True, decay=0.999,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        elif norm_type == 'group':\n",
    "            # normalize\n",
    "            # tranpose: [bs, h, w, c] to [bs, c, h, w] following the paper\n",
    "            x = tf.transpose(x, [0, 3, 1, 2])\n",
    "            N, C, H, W = x.get_shape().as_list()\n",
    "            G = min(G, C)\n",
    "            x = tf.reshape(x, [-1, G, C // G, H, W])   # <------------------------------这里源码错了 需要改成这样 https://github.com/shaohua0116/Group-Normalization-Tensorflow/issues/1\n",
    "            mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n",
    "            x = (x - mean) / tf.sqrt(var + esp)\n",
    "            # per channel gamma and beta\n",
    "            gamma = tf.get_variable('gamma', [C],\n",
    "                                    initializer=tf.constant_initializer(1.0))\n",
    "            beta = tf.get_variable('beta', [C],\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "            gamma = tf.reshape(gamma, [1, C, 1, 1])\n",
    "            beta = tf.reshape(beta, [1, C, 1, 1])\n",
    "\n",
    "            output = tf.reshape(x, [-1, C, H, W]) * gamma + beta   # 《------------------------ 这里源码错了 需要改成这样 https://github.com/shaohua0116/Group-Normalization-Tensorflow/issues/1\n",
    "            # tranpose: [bs, c, h, w, c] to [bs, h, w, c] following the paper\n",
    "            output = tf.transpose(output, [0, 2, 3, 1])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return output\n",
    "\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "    oshape = np.shape(batch[0])\n",
    "\n",
    "    if padding:\n",
    "        oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                      mode='constant', constant_values=0)\n",
    "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                    nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "\n",
    "def data_preprocessing(x_train,x_test):\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
    "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
    "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
    "\n",
    "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
    "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
    "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "img_channels = 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution layer Methods\n",
    "def __conv2d_p(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
    "               initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):\n",
    "    \"\"\"\n",
    "    Convolution 2D Wrapper\n",
    "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
    "    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n",
    "    :param w: (tf.tensor) pretrained weights (if None, it means no pretrained weights)\n",
    "    :param num_filters: (integer) No. of filters (This is the output depth)\n",
    "    :param kernel_size: (integer tuple) The size of the convolving kernel.\n",
    "    :param padding: (string) The amount of padding required.\n",
    "    :param stride: (integer tuple) The stride required.\n",
    "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
    "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
    "    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n",
    "    :return out: The output of the layer. (N, H', W', num_filters)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        stride = [1, stride[0], stride[1], 1]\n",
    "        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], num_filters]\n",
    "\n",
    "        with tf.name_scope('layer_weights'):\n",
    "            if w == None:\n",
    "                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n",
    "            __variable_summaries(w)\n",
    "        with tf.name_scope('layer_biases'):\n",
    "            if isinstance(bias, float):\n",
    "                bias = tf.get_variable('biases', [num_filters], initializer=tf.constant_initializer(bias))\n",
    "            __variable_summaries(bias)\n",
    "        with tf.name_scope('layer_conv2d'):\n",
    "            conv = tf.nn.conv2d(x, w, stride, padding)\n",
    "            out = tf.nn.bias_add(conv, bias)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def conv2d(name, x, w=None, num_filters=16, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
    "           initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0,\n",
    "           activation=None, batchnorm_enabled=False, max_pool_enabled=False, dropout_keep_prob=-1,\n",
    "           is_training=True):\n",
    "    \"\"\"\n",
    "    This block is responsible for a convolution 2D layer followed by optional (non-linearity, dropout, max-pooling).\n",
    "    Note that: \"is_training\" should be passed by a correct value based on being in either training or testing.\n",
    "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
    "    :param x: (tf.tensor) The input to the layer (N, H, W, C).\n",
    "    :param num_filters: (integer) No. of filters (This is the output depth)\n",
    "    :param kernel_size: (integer tuple) The size of the convolving kernel.\n",
    "    :param padding: (string) The amount of padding required.\n",
    "    :param stride: (integer tuple) The stride required.\n",
    "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
    "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
    "    :param bias: (float) Amount of bias.\n",
    "    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n",
    "    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n",
    "    :param max_pool_enabled:  (boolean) for enabling max-pooling 2x2 to decrease width and height by a factor of 2.\n",
    "    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n",
    "    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n",
    "    :return: The output tensor of the layer (N, H', W', C').\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        conv_o_b = __conv2d_p(scope, x=x, w=w, num_filters=num_filters, kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding,\n",
    "                              initializer=initializer, l2_strength=l2_strength, bias=bias)\n",
    "\n",
    "        if batchnorm_enabled:\n",
    "            conv_o_bn = instance_norm(conv_o_b,scope='instance_norm'+name) #   <--------------  instance\n",
    "#             conv_o_bn = norm(conv_o_b,norm_type='group',is_train = True,i=name) <--- Group\n",
    "            if not activation:\n",
    "                conv_a = conv_o_bn\n",
    "            else:\n",
    "                conv_a = activation(conv_o_bn)\n",
    "        else:\n",
    "            if not activation:\n",
    "                conv_a = conv_o_b\n",
    "            else:\n",
    "                conv_a = activation(conv_o_b)\n",
    "\n",
    "        def dropout_with_keep():\n",
    "            return tf.nn.dropout(conv_a, dropout_keep_prob)\n",
    "\n",
    "        def dropout_no_keep():\n",
    "            return tf.nn.dropout(conv_a, 1.0)\n",
    "\n",
    "        if dropout_keep_prob != -1:\n",
    "            conv_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
    "        else:\n",
    "            conv_o_dr = conv_a\n",
    "\n",
    "        conv_o = conv_o_dr\n",
    "        if max_pool_enabled:\n",
    "            conv_o = max_pool_2d(conv_o_dr)\n",
    "\n",
    "    return conv_o\n",
    "\n",
    "\n",
    "def __depthwise_conv2d_p(name, x, w=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0):\n",
    "    with tf.variable_scope(name):\n",
    "        stride = [1, stride[0], stride[1], 1]\n",
    "        kernel_shape = [kernel_size[0], kernel_size[1], x.shape[-1], 1]\n",
    "\n",
    "        with tf.name_scope('layer_weights'):\n",
    "            if w is None:\n",
    "                w = __variable_with_weight_decay(kernel_shape, initializer, l2_strength)\n",
    "            __variable_summaries(w)\n",
    "        with tf.name_scope('layer_biases'):\n",
    "            if isinstance(bias, float):\n",
    "                bias = tf.get_variable('biases', [x.shape[-1]], initializer=tf.constant_initializer(bias))\n",
    "            __variable_summaries(bias)\n",
    "        with tf.name_scope('layer_conv2d'):\n",
    "            conv = tf.nn.depthwise_conv2d(x, w, stride, padding)\n",
    "            out = tf.nn.bias_add(conv, bias)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def depthwise_conv2d(name, x, w=None, kernel_size=(3, 3), padding='SAME', stride=(1, 1),\n",
    "                     initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, bias=0.0, activation=None,\n",
    "                     batchnorm_enabled=False, is_training=True):\n",
    "    \"\"\"Implementation of depthwise 2D convolution wrapper\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        conv_o_b = __depthwise_conv2d_p(name=scope, x=x, w=w, kernel_size=kernel_size, padding=padding,\n",
    "                                        stride=stride, initializer=initializer, l2_strength=l2_strength, bias=bias)\n",
    "\n",
    "        if batchnorm_enabled:\n",
    "            conv_o_bn = tf.layers.batch_normalization(conv_o_b, training=is_training)\n",
    "            if not activation:\n",
    "                conv_a = conv_o_bn\n",
    "            else:\n",
    "                conv_a = activation(conv_o_bn)\n",
    "        else:\n",
    "            if not activation:\n",
    "                conv_a = conv_o_b\n",
    "            else:\n",
    "                conv_a = activation(conv_o_b)\n",
    "    return conv_a\n",
    "\n",
    "\n",
    "def depthwise_separable_conv2d(name, x, w_depthwise=None, w_pointwise=None, width_multiplier=1.0, num_filters=16,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding='SAME', stride=(1, 1),\n",
    "                               initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0, biases=(0.0, 0.0),\n",
    "                               activation=None, batchnorm_enabled=True,\n",
    "                               is_training=True):\n",
    "    \"\"\"Implementation of depthwise separable 2D convolution operator as in MobileNet paper\"\"\"\n",
    "    total_num_filters = int(round(num_filters * width_multiplier))\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        conv_a = depthwise_conv2d('depthwise', x=x, w=w_depthwise, kernel_size=kernel_size, padding=padding,\n",
    "                                  stride=stride,\n",
    "                                  initializer=initializer, l2_strength=l2_strength, bias=biases[0],\n",
    "                                  activation=activation,\n",
    "                                  batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n",
    "\n",
    "        conv_o = conv2d('pointwise', x=conv_a, w=w_pointwise, num_filters=total_num_filters, kernel_size=(1, 1),\n",
    "                        initializer=initializer, l2_strength=l2_strength, bias=biases[1], activation=activation,\n",
    "                        batchnorm_enabled=batchnorm_enabled, is_training=is_training)\n",
    "\n",
    "    return conv_a, conv_o\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Fully Connected layer Methods\n",
    "\n",
    "def __dense_p(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,\n",
    "              bias=0.0):\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
    "    :param x: (tf.tensor) The input to the layer (N, D).\n",
    "    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n",
    "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
    "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
    "    :param bias: (float) Amount of bias. (if not float, it means pretrained bias)\n",
    "    :return out: The output of the layer. (N, H)\n",
    "    \"\"\"\n",
    "    n_in = x.get_shape()[-1].value\n",
    "    with tf.variable_scope(name):\n",
    "        if w == None:\n",
    "            w = __variable_with_weight_decay([n_in, output_dim], initializer, l2_strength)\n",
    "        __variable_summaries(w)\n",
    "        if isinstance(bias, float):\n",
    "            bias = tf.get_variable(\"layer_biases\", [output_dim], tf.float32, tf.constant_initializer(bias))\n",
    "        __variable_summaries(bias)\n",
    "        output = tf.nn.bias_add(tf.matmul(x, w), bias)\n",
    "        return output\n",
    "\n",
    "\n",
    "def dense(name, x, w=None, output_dim=128, initializer=tf.contrib.layers.xavier_initializer(), l2_strength=0.0,\n",
    "          bias=0.0,\n",
    "          activation=None, batchnorm_enabled=False, dropout_keep_prob=-1,\n",
    "          is_training=True\n",
    "          ):\n",
    "    \"\"\"\n",
    "    This block is responsible for a fully connected followed by optional (non-linearity, dropout, max-pooling).\n",
    "    Note that: \"is_training\" should be passed by a correct value based on being in either training or testing.\n",
    "    :param name: (string) The name scope provided by the upper tf.name_scope('name') as scope.\n",
    "    :param x: (tf.tensor) The input to the layer (N, D).\n",
    "    :param output_dim: (integer) It specifies H, the output second dimension of the fully connected layer [ie:(N, H)]\n",
    "    :param initializer: (tf.contrib initializer) The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
    "    :param l2_strength:(weight decay) (float) L2 regularization parameter.\n",
    "    :param bias: (float) Amount of bias.\n",
    "    :param activation: (tf.graph operator) The activation function applied after the convolution operation. If None, linear is applied.\n",
    "    :param batchnorm_enabled: (boolean) for enabling batch normalization.\n",
    "    :param dropout_keep_prob: (float) for the probability of keeping neurons. If equals -1, it means no dropout\n",
    "    :param is_training: (boolean) to diff. between training and testing (important for batch normalization and dropout)\n",
    "    :return out: The output of the layer. (N, H)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        dense_o_b = __dense_p(name=scope, x=x, w=w, output_dim=output_dim, initializer=initializer,\n",
    "                              l2_strength=l2_strength,\n",
    "                              bias=bias)\n",
    "\n",
    "        if batchnorm_enabled:\n",
    "            dense_o_bn = tf.layers.batch_normalization(dense_o_b, training=is_training)\n",
    "            if not activation:\n",
    "                dense_a = dense_o_bn\n",
    "            else:\n",
    "                dense_a = activation(dense_o_bn)\n",
    "        else:\n",
    "            if not activation:\n",
    "                dense_a = dense_o_b\n",
    "            else:\n",
    "                dense_a = activation(dense_o_b)\n",
    "\n",
    "        def dropout_with_keep():\n",
    "            return tf.nn.dropout(dense_a, dropout_keep_prob)\n",
    "\n",
    "        def dropout_no_keep():\n",
    "            return tf.nn.dropout(dense_a, 1.0)\n",
    "\n",
    "        if dropout_keep_prob != -1:\n",
    "            dense_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
    "        else:\n",
    "            dense_o_dr = dense_a\n",
    "\n",
    "        dense_o = dense_o_dr\n",
    "    return dense_o\n",
    "\n",
    "\n",
    "def dropout(x, dropout_keep_prob, is_training):\n",
    "    \"\"\"Dropout special layer\"\"\"\n",
    "\n",
    "    def dropout_with_keep():\n",
    "        return tf.nn.dropout(x, dropout_keep_prob)\n",
    "\n",
    "    def dropout_no_keep():\n",
    "        return tf.nn.dropout(x, 1.0)\n",
    "\n",
    "    if dropout_keep_prob != -1:\n",
    "        output = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\n",
    "    else:\n",
    "        output = x\n",
    "    return output\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"\n",
    "    Flatten a (N,H,W,C) input into (N,D) output. Used for fully connected layers after conolution layers\n",
    "    :param x: (tf.tensor) representing input\n",
    "    :return: flattened output\n",
    "    \"\"\"\n",
    "    all_dims_exc_first = np.prod([v.value for v in x.get_shape()[1:]])\n",
    "    o = tf.reshape(x, [-1, all_dims_exc_first])\n",
    "    return o\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Pooling Methods\n",
    "\n",
    "def max_pool_2d(x, size=(2, 2), stride=(2, 2), name='pooling'):\n",
    "    \"\"\"\n",
    "    Max pooling 2D Wrapper\n",
    "    :param x: (tf.tensor) The input to the layer (N,H,W,C).\n",
    "    :param size: (tuple) This specifies the size of the filter as well as the stride.\n",
    "    :param stride: (tuple) specifies the stride of pooling.\n",
    "    :param name: (string) Scope name.\n",
    "    :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n",
    "    \"\"\"\n",
    "    size_x, size_y = size\n",
    "    stride_x, stride_y = stride\n",
    "    return tf.nn.max_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding='VALID',\n",
    "                          name=name)\n",
    "\n",
    "\n",
    "def avg_pool_2d(x, size=(2, 2), stride=(2, 2), name='avg_pooling'):\n",
    "    \"\"\"\n",
    "        Average pooling 2D Wrapper\n",
    "        :param x: (tf.tensor) The input to the layer (N,H,W,C).\n",
    "        :param size: (tuple) This specifies the size of the filter as well as the stride.\n",
    "        :param name: (string) Scope name.\n",
    "        :return: The output is the same input but halfed in both width and height (N,H/2,W/2,C).\n",
    "    \"\"\"\n",
    "    size_x, size_y = size\n",
    "    stride_x, stride_y = stride\n",
    "    return tf.nn.avg_pool(x, ksize=[1, size_x, size_y, 1], strides=[1, stride_x, stride_y, 1], padding='VALID',\n",
    "                          name=name)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Utilities for layers\n",
    "\n",
    "def __variable_with_weight_decay(kernel_shape, initializer, wd):\n",
    "    \"\"\"\n",
    "    Create a variable with L2 Regularization (Weight Decay)\n",
    "    :param kernel_shape: the size of the convolving weight kernel.\n",
    "    :param initializer: The initialization scheme, He et al. normal or Xavier normal are recommended.\n",
    "    :param wd:(weight decay) L2 regularization parameter.\n",
    "    :return: The weights of the kernel initialized. The L2 loss is added to the loss collection.\n",
    "    \"\"\"\n",
    "    w = tf.get_variable('weights', kernel_shape, tf.float32, initializer=initializer)\n",
    "\n",
    "    collection_name = tf.GraphKeys.REGULARIZATION_LOSSES\n",
    "    if wd and (not tf.get_variable_scope().reuse):\n",
    "        weight_decay = tf.multiply(tf.nn.l2_loss(w), wd, name='w_loss')\n",
    "        tf.add_to_collection(collection_name, weight_decay)\n",
    "    return w\n",
    "\n",
    "\n",
    "# Summaries for variables\n",
    "def __variable_summaries(var):\n",
    "    \"\"\"\n",
    "    Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "    :param var: variable to be summarized\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo,encoding='latin1')\n",
    "    return dict\n",
    "\n",
    "def load_data_one(file):\n",
    "    batch = unpickle(file)\n",
    "    data = batch['data']\n",
    "    labels = batch['labels']\n",
    "    print(\"Loading %s : %d.\" % (file, len(data)))\n",
    "    return data, labels\n",
    "\n",
    "def load_data(files, data_dir, label_count):\n",
    "    global image_size, img_channels\n",
    "    data, labels = load_data_one(data_dir + '/' + files[0])\n",
    "    for f in files[1:]:\n",
    "        data_n, labels_n = load_data_one(data_dir + '/' + f)\n",
    "        data = np.append(data, data_n, axis=0)\n",
    "        labels = np.append(labels, labels_n, axis=0)\n",
    "    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n",
    "    data = data.reshape([-1, img_channels, image_size, image_size])\n",
    "    print('dataShape is ----------------:')\n",
    "    print(data.shape)\n",
    "    data = data.transpose([0, 2, 3, 1])\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    print(\"======Loading data======\")\n",
    "    data_dir = './cifar-10-python/cifar-10-batches-py'\n",
    "    image_dim = image_size * image_size * img_channels\n",
    "    meta = unpickle(data_dir + '/batches.meta')\n",
    "\n",
    "    label_names = meta['label_names']\n",
    "    label_count = len(label_names)\n",
    "    train_files = ['data_batch_%d' % d for d in range(1, 6)]\n",
    "    train_data, train_labels = load_data(train_files, data_dir, label_count)\n",
    "    test_data, test_labels = load_data(['test_batch'], data_dir, label_count)\n",
    "\n",
    "    print(\"Train data:\", np.shape(train_data), np.shape(train_labels))\n",
    "    print(\"Test data :\", np.shape(test_data), np.shape(test_labels))\n",
    "    print(\"======Load finished======\")\n",
    "\n",
    "    print(\"======Shuffling data======\")\n",
    "    indices = np.random.permutation(len(train_data))\n",
    "    train_data = train_data[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "    print(\"======Prepare Finished======\")\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(input):\n",
    "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3,\n",
    "                                        is_training=train_flag, updates_collections=None)\n",
    "\n",
    "\n",
    "def instance_norm(x, scope='instance_norm'):\n",
    "    return tf_contrib.layers.instance_norm(x,\n",
    "                                           epsilon=1e-05,\n",
    "                                           center=True, scale=True,\n",
    "                                           scope=scope)\n",
    "\n",
    "def layer_norm(x, scope='layer_norm') :\n",
    "    return tf_contrib.layers.layer_norm(x,\n",
    "                                        center=True, scale=True,\n",
    "                                        scope=scope)\n",
    "\n",
    "def norm(x, norm_type, is_train,i, G=32, esp=1e-5):\n",
    "    with tf.variable_scope('{}_norm_{}'.format(norm_type,i)):\n",
    "        if norm_type == 'none':\n",
    "            output = x\n",
    "        elif norm_type == 'batch':\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "                x, center=True, scale=True, decay=0.999,\n",
    "                is_training=is_train, updates_collections=None\n",
    "            )\n",
    "        elif norm_type == 'group':\n",
    "            # normalize\n",
    "            # tranpose: [bs, h, w, c] to [bs, c, h, w] following the paper\n",
    "            x = tf.transpose(x, [0, 3, 1, 2])\n",
    "            N, C, H, W = x.get_shape().as_list()\n",
    "            G = min(G, C)\n",
    "            x = tf.reshape(x, [-1, G, C // G, H, W])   # <------------------------------这里源码错了 需要改成这样 https://github.com/shaohua0116/Group-Normalization-Tensorflow/issues/1\n",
    "            mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)\n",
    "            x = (x - mean) / tf.sqrt(var + esp)\n",
    "            # per channel gamma and beta\n",
    "            gamma = tf.get_variable('gamma', [C],\n",
    "                                    initializer=tf.constant_initializer(1.0))\n",
    "            beta = tf.get_variable('beta', [C],\n",
    "                                   initializer=tf.constant_initializer(0.0))\n",
    "            gamma = tf.reshape(gamma, [1, C, 1, 1])\n",
    "            beta = tf.reshape(beta, [1, C, 1, 1])\n",
    "\n",
    "            output = tf.reshape(x, [-1, C, H, W]) * gamma + beta   # 《------------------------ 这里源码错了 需要改成这样 https://github.com/shaohua0116/Group-Normalization-Tensorflow/issues/1\n",
    "            # tranpose: [bs, c, h, w, c] to [bs, h, w, c] following the paper\n",
    "            output = tf.transpose(output, [0, 2, 3, 1])\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return output\n",
    "\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "    oshape = np.shape(batch[0])\n",
    "\n",
    "    if padding:\n",
    "        oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "    new_batch = []\n",
    "    npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "    for i in range(len(batch)):\n",
    "        new_batch.append(batch[i])\n",
    "        if padding:\n",
    "            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                      mode='constant', constant_values=0)\n",
    "        nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "        nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                    nw:nw + crop_shape[1]]\n",
    "    return new_batch\n",
    "\n",
    "\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "\n",
    "def data_preprocessing(x_train,x_test):\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n",
    "    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n",
    "    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n",
    "\n",
    "    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n",
    "    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n",
    "    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    batch = _random_flip_leftright(batch)\n",
    "    batch = _random_crop(batch, [32, 32], 4)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __init_network():\n",
    "    train_x, train_y, test_x, test_y = prepare_data()\n",
    "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
    "\n",
    "    \n",
    "    num_classes = 10\n",
    "    with tf.variable_scope('global_epoch'):\n",
    "        global_epoch_tensor = tf.Variable(-1, trainable=False, name='global_epoch')\n",
    "        global_epoch_input = tf.placeholder('int32', None, name='global_epoch_input')\n",
    "        global_epoch_assign_op = global_epoch_tensor.assign(global_epoch_input)\n",
    "    with tf.variable_scope('global_step'):\n",
    "        global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "        global_step_input = tf.placeholder('int32', None, name='global_step_input')\n",
    "        global_step_assign_op = global_step_tensor.assign(global_step_input)\n",
    "\n",
    "    with tf.variable_scope('input'):\n",
    "        # Input images\n",
    "        X = tf.placeholder(tf.float32,\n",
    "                                [None, image_size, image_size,3])\n",
    "        # Classification supervision, it's an argmax. Feel free to change it to one-hot,\n",
    "        # but don't forget to change the loss from sparse as well\n",
    "        y = tf.placeholder(tf.float32, [None,num_classes])   ### <---------------------------注意一下\n",
    "        # is_training is for batch normalization and dropout, if they exist\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        # 改变x的格式转为4D的向量[batch, in_height, in_width, in_channels]`\n",
    "        X = tf.reshape(X, [-1, image_size, image_size, 3], name='x_image')\n",
    "        input_layer = tf.image.resize_images(X, [224, 224])\n",
    "    with tf.variable_scope('mobilenet_encoder'):\n",
    "        conv1_1 = conv2d('conv_1', input_layer, num_filters=int(round(32 * width_multiplier)),\n",
    "                             kernel_size=(3, 3),\n",
    "                             padding='SAME', stride=(2, 2), activation=tf.nn.relu6,\n",
    "                             batchnorm_enabled=batchnorm_enabled,\n",
    "                             is_training=is_training, l2_strength=l2_strength, bias=bias)\n",
    "#         __add_to_nodes([conv1_1])\n",
    "        ############################################################################################\n",
    "        print(conv1_1)\n",
    "        conv2_1_dw, conv2_1_pw = depthwise_separable_conv2d('conv_ds_2', conv1_1,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=64, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv2_1_dw, conv2_1_pw])\n",
    "        print(conv2_1_pw)\n",
    "        conv2_2_dw, conv2_2_pw = depthwise_separable_conv2d('conv_ds_3', conv2_1_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=128, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(2, 2),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv2_2_dw, conv2_2_pw])\n",
    "        print(conv2_2_pw)\n",
    "        ############################################################################################\n",
    "        conv3_1_dw, conv3_1_pw = depthwise_separable_conv2d('conv_ds_4', conv2_2_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=128, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv3_1_dw, conv3_1_pw])\n",
    "        print(conv3_1_pw)\n",
    "        conv3_2_dw, conv3_2_pw = depthwise_separable_conv2d('conv_ds_5', conv3_1_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=256, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(2, 2),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv3_2_dw, conv3_2_pw])\n",
    "        print(conv3_2_pw)\n",
    "        ############################################################################################\n",
    "        conv4_1_dw, conv4_1_pw = depthwise_separable_conv2d('conv_ds_6', conv3_2_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=256, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv4_1_dw, conv4_1_pw])\n",
    "        print(conv4_1_pw)\n",
    "        conv4_2_dw, conv4_2_pw = depthwise_separable_conv2d('conv_ds_7', conv4_1_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(2, 2),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv4_2_dw, conv4_2_pw])\n",
    "        print(conv4_2_pw)\n",
    "        ############################################################################################\n",
    "        conv5_1_dw, conv5_1_pw = depthwise_separable_conv2d('conv_ds_8', conv4_2_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_1_dw, conv5_1_pw])\n",
    "        print(conv5_1_pw)\n",
    "        conv5_2_dw, conv5_2_pw = depthwise_separable_conv2d('conv_ds_9', conv5_1_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_2_dw, conv5_2_pw])\n",
    "        print(conv5_2_pw)\n",
    "        conv5_3_dw, conv5_3_pw = depthwise_separable_conv2d('conv_ds_10', conv5_2_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_3_dw, conv5_3_pw])\n",
    "        print(conv5_3_pw)\n",
    "        conv5_4_dw, conv5_4_pw = depthwise_separable_conv2d('conv_ds_11', conv5_3_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_4_dw, conv5_4_pw])\n",
    "        print(conv5_4_pw)\n",
    "        conv5_5_dw, conv5_5_pw = depthwise_separable_conv2d('conv_ds_12', conv5_4_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=512, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_5_dw, conv5_5_pw])\n",
    "        print(conv5_5_pw)\n",
    "        conv5_6_dw, conv5_6_pw = depthwise_separable_conv2d('conv_ds_13', conv5_5_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=1024, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(2, 2),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv5_6_dw, conv5_6_pw])\n",
    "        print(conv5_6_pw)\n",
    "        ############################################################################################\n",
    "        conv6_1_dw, conv6_1_pw = depthwise_separable_conv2d('conv_ds_14', conv5_6_pw,\n",
    "                                                                width_multiplier=width_multiplier,\n",
    "                                                                num_filters=1024, kernel_size=(3, 3), padding='SAME',\n",
    "                                                                stride=(1, 1),\n",
    "                                                                batchnorm_enabled=batchnorm_enabled,\n",
    "                                                                activation=tf.nn.relu6,\n",
    "                                                                is_training=is_training,\n",
    "                                                                l2_strength=l2_strength,\n",
    "                                                                biases=(bias, bias))\n",
    "#         __add_to_nodes([conv6_1_dw, conv6_1_pw])\n",
    "        print(conv6_1_pw)\n",
    "        ############################################################################################\n",
    "        avg_pool = avg_pool_2d(conv6_1_pw, size=(7, 7), stride=(1, 1),name=\"avg_pool\")  ## <----------------------这里改了核大小\n",
    "        print(avg_pool)\n",
    "        dropped = tf.nn.dropout(avg_pool, dropout_keep_prob)\n",
    "        print(dropped)\n",
    "#         dropped = dropout(avg_pool, 0.999, is_training)\n",
    "        logits = flatten(conv2d('fc', dropped, kernel_size=(1, 1), num_filters=num_classes,\n",
    "                                         l2_strength=l2_strength,\n",
    "                                         bias=bias))\n",
    "        print(logits)\n",
    "#         return logits\n",
    "#         __add_to_nodes([avg_pool, dropped, logits])\n",
    "    with tf.variable_scope('output'):\n",
    "        regularization_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "        cross_entropy_loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=tf.argmax(tf.cast(y, dtype=tf.int32),1), name='loss'))\n",
    "        loss = regularization_loss + cross_entropy_loss\n",
    "\n",
    "        # Important for Batch Normalization\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "            y_out_argmax = tf.argmax(tf.nn.softmax(logits), axis=-1, output_type=tf.int64)\n",
    "\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.cast(y,dtype=tf.int64), -1), y_out_argmax), tf.float32))\n",
    "\n",
    "        # Summaries needed for TensorBoard\n",
    "        with tf.name_scope('train-summary-per-iteration'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            tf.summary.scalar('acc', accuracy)\n",
    "            summaries_merged = tf.summary.merge_all()\n",
    "    #保存模型使用环境\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # 创建一个协调器，管理线程\n",
    "        coord = tf.train.Coordinator()\n",
    "        # 启动QueueRunner, 此时文件名队列已经进队\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        for ep in range(1, total_epoch+1):\n",
    "            train_acc = 0.0\n",
    "            pre_index = 0\n",
    "            Train_acc_all = 0.0\n",
    "            print(\"\\n epoch %d/%d:\" % (ep, total_epoch))\n",
    "            for i in range(1,iterations+1):\n",
    "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
    "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
    "\n",
    "                batch_x_test = test_x[pre_index:pre_index+batch_size]\n",
    "                batch_y_test = test_y[pre_index:pre_index+batch_size]\n",
    "                \n",
    "                batch_x = data_augmentation(batch_x)\n",
    "\n",
    "\n",
    "                # 训练模型\n",
    "                sess.run(train_op, feed_dict={X: batch_x, y: batch_y,is_training:True})\n",
    "                \n",
    "                test_acc = sess.run(accuracy, feed_dict={X: batch_x_test, y: batch_y_test,is_training:True})\n",
    "                train_acc = sess.run(accuracy, feed_dict={X: batch_x, y: batch_y,is_training:True})\n",
    "                print(\"训练第 \" + str(i) + \" 次, 训练集准确率= \" + str(train_acc) + \" , 测试集准确率= \" + str(test_acc))\n",
    "\n",
    "                Train_acc_all += train_acc\n",
    "                pre_index += batch_size\n",
    "                \n",
    "                print(\"iteration: %d/%d, train_acc: %.4f\"\n",
    "                          % (i, iterations, Train_acc_all / i ))\n",
    "                \n",
    "\n",
    "                if test_acc == 1 and train_acc >= 0.95:\n",
    "                    print(\"准确率完爆了\")\n",
    "                    # 保存模型\n",
    "                    saver.save(sess, 'nn/my_net.ckpt')\n",
    "                    break\n",
    "\n",
    "        # 通知其他线程关闭\n",
    "        coord.request_stop()\n",
    "        # 其他所有线程关闭之后，这一函数才能返回\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Loading data======\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/data_batch_5 : 10000.\n",
      "dataShape is ----------------:\n",
      "(50000, 3, 32, 32)\n",
      "Loading ./cifar-10-python/cifar-10-batches-py/test_batch : 10000.\n",
      "dataShape is ----------------:\n",
      "(10000, 3, 32, 32)\n",
      "Train data: (50000, 32, 32, 3) (50000, 10)\n",
      "Test data : (10000, 32, 32, 3) (10000, 10)\n",
      "======Load finished======\n",
      "======Shuffling data======\n",
      "======Prepare Finished======\n",
      "Tensor(\"mobilenet_encoder/conv_1/Relu6:0\", shape=(?, 112, 112, 32), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_2/pointwise/Relu6:0\", shape=(?, 112, 112, 64), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_3/pointwise/Relu6:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_4/pointwise/Relu6:0\", shape=(?, 56, 56, 128), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_5/pointwise/Relu6:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_6/pointwise/Relu6:0\", shape=(?, 28, 28, 256), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_7/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_8/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_9/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_10/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_11/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_12/pointwise/Relu6:0\", shape=(?, 14, 14, 512), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_13/pointwise/Relu6:0\", shape=(?, 7, 7, 1024), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/conv_ds_14/pointwise/Relu6:0\", shape=(?, 7, 7, 1024), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/avg_pool:0\", shape=(?, 1, 1, 1024), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/dropout/mul:0\", shape=(?, 1, 1, 1024), dtype=float32)\n",
      "Tensor(\"mobilenet_encoder/Reshape:0\", shape=(?, 10), dtype=float32)\n",
      "\n",
      " epoch 1/1:\n",
      "训练第 1 次, 训练集准确率= 0.1875 , 测试集准确率= 0.1875\n",
      "iteration: 1/10, train_acc: 0.1875\n",
      "训练第 2 次, 训练集准确率= 0.125 , 测试集准确率= 0.0625\n",
      "iteration: 2/10, train_acc: 0.1562\n",
      "训练第 3 次, 训练集准确率= 0.125 , 测试集准确率= 0.0\n",
      "iteration: 3/10, train_acc: 0.1458\n",
      "训练第 4 次, 训练集准确率= 0.1875 , 测试集准确率= 0.0625\n",
      "iteration: 4/10, train_acc: 0.1562\n",
      "训练第 5 次, 训练集准确率= 0.125 , 测试集准确率= 0.0\n",
      "iteration: 5/10, train_acc: 0.1500\n",
      "训练第 6 次, 训练集准确率= 0.1875 , 测试集准确率= 0.0625\n",
      "iteration: 6/10, train_acc: 0.1562\n",
      "训练第 7 次, 训练集准确率= 0.125 , 测试集准确率= 0.1875\n",
      "iteration: 7/10, train_acc: 0.1518\n",
      "训练第 8 次, 训练集准确率= 0.1875 , 测试集准确率= 0.125\n",
      "iteration: 8/10, train_acc: 0.1562\n",
      "训练第 9 次, 训练集准确率= 0.25 , 测试集准确率= 0.0\n",
      "iteration: 9/10, train_acc: 0.1667\n",
      "训练第 10 次, 训练集准确率= 0.1875 , 测试集准确率= 0.125\n",
      "iteration: 10/10, train_acc: 0.1688\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    __init_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python conda_py36",
   "language": "python",
   "name": "conda_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
